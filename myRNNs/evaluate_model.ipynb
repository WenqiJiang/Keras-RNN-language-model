{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# from IPython.display import HTML\n",
    "\n",
    "# InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "# from keras import Model\n",
    "# from keras.models import load_model\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore', category = RuntimeWarning)\n",
    "# warnings.filterwarnings('ignore', category = UserWarning)\n",
    "\n",
    "# BATCH_SIZE = 2048\n",
    "# RANDOM_STATE = 50\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from utils import get_model, find_closest, get_sequences, create_train_valid,  generate_output, guess_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up IPython to show all outputs from a cell\n",
    "import warnings\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "RANDOM_STATE = 50\n",
    "EPOCHS = 150\n",
    "BATCH_SIZE = 2048\n",
    "TRAINING_LENGTH = 50\n",
    "TRAIN_FRACTION = 0.7\n",
    "VERBOSE = 0\n",
    "SAVE_MODEL = True\n",
    "RNN_CELLS = 128\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read in data\n",
    "data = pd.read_csv(\n",
    "    '../data/neural_network_patent_query.csv', parse_dates=['patent_date'])\n",
    "\n",
    "# Extract abstracts\n",
    "original_abstracts = list(data['patent_abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences(texts,\n",
    "                   training_length=50,\n",
    "                   lower=True,\n",
    "                   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
    "    \"\"\"Turn a set of texts into sequences of integers\"\"\"\n",
    "\n",
    "    # Create the tokenizer object and train on texts\n",
    "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    # Create look-up dictionaries and reverse look-ups\n",
    "    word_idx = tokenizer.word_index\n",
    "    idx_word = tokenizer.index_word\n",
    "    num_words = len(word_idx) + 1\n",
    "    word_counts = tokenizer.word_counts\n",
    "\n",
    "    print(f'There are {num_words} unique words.')\n",
    "\n",
    "    # Convert text to sequences of integers\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    # Limit to sequences with more than training length tokens\n",
    "    seq_lengths = [len(x) for x in sequences]\n",
    "    over_idx = [\n",
    "        i for i, l in enumerate(seq_lengths) if l > (training_length + 20)\n",
    "    ]\n",
    "\n",
    "    new_texts = []\n",
    "    new_sequences = []\n",
    "\n",
    "    # Only keep sequences with more than training length tokens\n",
    "    for i in over_idx:\n",
    "        new_texts.append(texts[i])\n",
    "        new_sequences.append(sequences[i])\n",
    "\n",
    "    training_seq = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate through the sequences of tokens\n",
    "    for seq in new_sequences:\n",
    "\n",
    "        # Create multiple training examples from each sequence\n",
    "        for i in range(training_length, len(seq)):\n",
    "            # Extract the features and label\n",
    "            extract = seq[i - training_length:i + 1]\n",
    "\n",
    "            # Set the features and label\n",
    "            training_seq.append(extract[:-1])\n",
    "            labels.append(extract[-1])\n",
    "\n",
    "    print(f'There are {len(training_seq)} training sequences.')\n",
    "\n",
    "    # Return everything needed for setting up the model\n",
    "    return word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, training_seq, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'this is a short sentence 1 with one reference to an image this next sentence while non sensical does not have an image and has two commas'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "example = 'This is a short sentence (1) with one reference to an image. This next sentence, while non-sensical, does not have an image and has two commas.'\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts([example])\n",
    "s = tokenizer.texts_to_sequences([example])[0]\n",
    "' '.join(tokenizer.index_word[i] for i in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a short sentence (1) with one reference to an image. this next sentence, while non-sensical, does not have an image and has two commas.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['this', 'an', 'is', 'a', 'short', 'sentence', '(1)', 'with', 'one', 'reference', 'to', 'image.', 'next', 'sentence,', 'while', 'non-sensical,', 'does', 'not', 'have', 'image', 'and', 'has', 'two', 'commas.'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(filters='\"#$%&*+/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts([example])\n",
    "s = tokenizer.texts_to_sequences([example])[0]\n",
    "' '.join(tokenizer.index_word[i] for i in s)\n",
    "tokenizer.word_index.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a short sentence with one reference to an image . This next sentence , while non-sensical , does not have an image and has two commas .'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def format_patent(patent):\n",
    "    \"\"\"Add spaces around punctuation and remove references to images/citations.\"\"\"\n",
    "\n",
    "    # Add spaces around punctuation\n",
    "    patent = re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', patent)\n",
    "\n",
    "    # Remove references to figures\n",
    "    patent = re.sub(r'\\((\\d+)\\)', r'', patent)\n",
    "\n",
    "    # Remove double spaces\n",
    "    patent = re.sub(r'\\s\\s', ' ', patent)\n",
    "    return patent\n",
    "\n",
    "\n",
    "f = format_patent(example)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3522"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted = []\n",
    "\n",
    "# Iterate through all the original abstracts\n",
    "for a in original_abstracts:\n",
    "    formatted.append(format_patent(a))\n",
    "\n",
    "len(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16192 unique words.\n",
      "There are 318563 training sequences.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_LENGTH = 50\n",
    "\n",
    "filters = '!\"%;[\\\\]^_`{|}~\\t\\n'\n",
    "word_idx, idx_word, num_words, word_counts, abstracts, sequences, features, labels = make_sequences(\n",
    "    formatted, TRAINING_LENGTH, lower=False, filters=filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 30760),\n",
       " ('a', 21442),\n",
       " ('of', 20157),\n",
       " ('.', 16554),\n",
       " (',', 15415),\n",
       " ('and', 12563),\n",
       " ('to', 12012),\n",
       " ('network', 7618),\n",
       " ('neural', 7235),\n",
       " ('is', 7211),\n",
       " ('for', 6779),\n",
       " ('in', 6131),\n",
       " ('The', 5813),\n",
       " ('an', 5286),\n",
       " ('data', 3971)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "def create_train_valid(features,\n",
    "                       labels,\n",
    "                       num_words,\n",
    "                       train_fraction=TRAIN_FRACTION):\n",
    "    \"\"\"Create training and validation features and labels.\"\"\"\n",
    "\n",
    "    # Randomly shuffle features and labels\n",
    "    features, labels = shuffle(features, labels, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Decide on number of samples for training\n",
    "    train_end = int(train_fraction * len(labels))\n",
    "\n",
    "    train_features = np.array(features[:train_end])\n",
    "    valid_features = np.array(features[train_end:])\n",
    "\n",
    "    train_labels = labels[:train_end]\n",
    "    valid_labels = labels[train_end:]\n",
    "\n",
    "    # Convert to arrays\n",
    "    X_train, X_valid = np.array(train_features), np.array(valid_features)\n",
    "\n",
    "    # Using int8 for memory savings\n",
    "    y_train = np.zeros((len(train_labels), num_words), dtype=np.int8)\n",
    "    y_valid = np.zeros((len(valid_labels), num_words), dtype=np.int8)\n",
    "\n",
    "    # One hot encoding of labels\n",
    "    for example_index, word_index in enumerate(train_labels):\n",
    "        y_train[example_index, word_index] = 1\n",
    "\n",
    "    for example_index, word_index in enumerate(valid_labels):\n",
    "        y_valid[example_index, word_index] = 1\n",
    "\n",
    "    # Memory management\n",
    "    import gc\n",
    "    gc.enable()\n",
    "    del features, labels, train_features, valid_features, train_labels, valid_labels\n",
    "    gc.collect()\n",
    "\n",
    "    return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 101)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array(['the', '-0.038194', '-0.24487', '0.72812', '-0.39961', '0.083172',\n",
       "       '0.043953', '-0.39141', '0.3344', '-0.57545', '0.087459',\n",
       "       '0.28787', '-0.06731', '0.30906', '-0.26384', '-0.13231',\n",
       "       '-0.20757', '0.33395', '-0.33848', '-0.31743', '-0.48336',\n",
       "       '0.1464', '-0.37304', '0.34577', '0.052041', '0.44946', '-0.46971',\n",
       "       '0.02628', '-0.54155', '-0.15518', '-0.14107', '-0.039722',\n",
       "       '0.28277', '0.14393', '0.23464', '-0.31021', '0.086173', '0.20397',\n",
       "       '0.52624', '0.17164', '-0.082378', '-0.71787', '-0.41531',\n",
       "       '0.20335', '-0.12763', '0.41367', '0.55187', '0.57908', '-0.33477',\n",
       "       '-0.36559', '-0.54857', '-0.062892', '0.26584', '0.30205',\n",
       "       '0.99775', '-0.80481', '-3.0243', '0.01254', '-0.36942', '2.2167',\n",
       "       '0.72201', '-0.24978', '0.92136', '0.034514', '0.46745', '1.1079',\n",
       "       '-0.19358', '-0.074575', '0.23353', '-0.052062', '-0.22044',\n",
       "       '0.057162', '-0.15806', '-0.30798', '-0.41625', '0.37972',\n",
       "       '0.15006', '-0.53212', '-0.2055', '-1.2526', '0.071624', '0.70565',\n",
       "       '0.49744', '-0.42063', '0.26148', '-1.538', '-0.30223',\n",
       "       '-0.073438', '-0.28312', '0.37104', '-0.25217', '0.016215',\n",
       "       '-0.017099', '-0.38984', '0.87424', '-0.72569', '-0.51058',\n",
       "       '-0.52028', '-0.1459', '0.8278', '0.27062'], dtype='<U22')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from keras.utils import get_file\n",
    "\n",
    "# Vectors to use\n",
    "glove_vectors = '/home/jwq/.keras/datasets/glove.6B.zip'\n",
    "\n",
    "# Download word embeddings if they are not present\n",
    "if not os.path.exists(glove_vectors):\n",
    "    glove_vectors = get_file('glove.6B.zip',\n",
    "                             'http://nlp.stanford.edu/data/glove.6B.zip')\n",
    "    os.system(f'unzip {glove_vectors}')\n",
    "\n",
    "# Load in unzipped file\n",
    "glove_vectors = '/home/jwq/.keras/datasets/glove.6B.100d.txt'\n",
    "glove = np.loadtxt(glove_vectors, dtype='str', comments=None)\n",
    "glove.shape\n",
    "glove[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-3.9551e-01,  5.4660e-01,  5.0315e-01, -6.3682e-01, -4.5470e-01,\n",
       "         3.0889e-01, -4.9240e-02,  2.7191e-01,  3.1562e-01, -3.2879e-01,\n",
       "         2.5089e-01,  1.4508e-01,  3.5136e-01, -2.2793e-01, -1.5894e-01,\n",
       "        -5.1527e-01, -2.7978e-01,  3.6470e-01, -3.9425e-01,  3.3299e-01,\n",
       "         4.3051e-01,  1.8300e-01,  2.5095e-01, -1.8547e-01,  3.4698e-01,\n",
       "         5.5137e-02, -4.5979e-01, -8.2963e-01, -1.8523e-02, -3.6772e-01,\n",
       "         4.5566e-02,  7.1052e-01, -2.2782e-02, -8.0889e-02,  2.0685e-01,\n",
       "         4.9855e-01, -5.9794e-02, -8.0048e-03, -2.3823e-01, -3.3759e-01,\n",
       "        -2.4201e-01, -2.3788e-01, -1.1362e-03, -4.0395e-01, -4.4859e-01,\n",
       "        -3.2189e-01,  4.8405e-01, -2.7999e-02,  1.0148e-01, -9.3585e-01,\n",
       "        -8.7522e-02, -3.9959e-01,  3.6545e-01,  1.3726e+00, -3.0713e-01,\n",
       "        -2.5940e+00,  2.2431e-01, -4.1168e-02,  1.7765e+00,  4.0010e-01,\n",
       "        -1.0996e-01,  1.4178e+00, -2.6154e-01,  1.8617e-01,  7.9328e-01,\n",
       "        -1.1709e-01,  8.7541e-01,  4.3911e-01,  3.4711e-01, -2.8515e-01,\n",
       "         7.6269e-02, -6.3038e-01,  1.6408e-01, -3.7053e-01,  5.8485e-01,\n",
       "        -1.5472e-01, -2.6382e-01, -1.8590e-01, -7.5228e-01, -1.5752e-01,\n",
       "         7.8539e-01, -1.8846e-02, -8.0130e-01,  1.5561e-01, -1.8624e+00,\n",
       "        -1.6969e-01,  1.9419e-01, -3.0683e-01, -7.8067e-01, -4.9689e-01,\n",
       "        -1.8256e-01, -4.2016e-02, -2.6290e-01,  5.8531e-02, -4.4664e-01,\n",
       "        -9.9765e-02, -4.3050e-01, -2.3693e-01, -1.4519e-02,  3.1981e-01]),\n",
       " 'so')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = glove[:, 1:].astype('float')\n",
    "words = glove[:, 0]\n",
    "\n",
    "del glove\n",
    "\n",
    "vectors[100], words[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 6317 words without pre-trained embeddings.\n"
     ]
    }
   ],
   "source": [
    "word_lookup = {word: vector for word, vector in zip(words, vectors)}\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, vectors.shape[1]))\n",
    "\n",
    "not_found = 0\n",
    "\n",
    "for i, word in enumerate(word_idx.keys()):\n",
    "    # Look up the word embedding\n",
    "    vector = word_lookup.get(word, None)\n",
    "\n",
    "    # Record in matrix\n",
    "    if vector is not None:\n",
    "        embedding_matrix[i + 1, :] = vector\n",
    "    else:\n",
    "        not_found += 1\n",
    "\n",
    "print(f'There were {not_found} words without pre-trained embeddings.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 6317 words without pre-trained embeddings.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16192, 100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((num_words, len(word_lookup['the'])))\n",
    "\n",
    "not_found = 0\n",
    "\n",
    "for i, word in enumerate(word_idx.keys()):\n",
    "    # Look up the word embedding\n",
    "    vector = word_lookup.get(word, None)\n",
    "\n",
    "    # Record in matrix\n",
    "    if vector is not None:\n",
    "        embedding_matrix[i + 1, :] = vector\n",
    "    else:\n",
    "        not_found += 1\n",
    "\n",
    "print(f'There were {not_found} words without pre-trained embeddings.')\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((222994, 50), (222994, 16192))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into training and validation\n",
    "X_train, X_valid, y_train, y_valid = create_train_valid(\n",
    "    features, labels, num_words)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.61071896"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object: y_train   \tSize: 3.61071896 GB.\n",
      "Object: y_valid   \tSize: 1.54745336 GB.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(y_train) / 1e9\n",
    "\n",
    "def check_sizes(gb_min=1):\n",
    "    for x in globals():\n",
    "        size = sys.getsizeof(eval(x)) / 1e9\n",
    "        if size > gb_min:\n",
    "            print(f'Object: {x:10}\\tSize: {size} GB.')\n",
    "\n",
    "check_sizes(gb_min=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         1619200   \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, None, 100)         0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 128)               29312     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16192)             2088768   \n",
      "=================================================================\n",
      "Total params: 3,737,280\n",
      "Trainable params: 2,118,080\n",
      "Non-trainable params: 1,619,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional, SimpleRNN\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "def make_word_level_model(num_words,\n",
    "                          embedding_matrix,\n",
    "                          rnn_cells=128,\n",
    "                          trainable=False,\n",
    "                          rnn_layers=1,\n",
    "                          bi_direc=False):\n",
    "    \"\"\"Make a word level recurrent neural network with option for pretrained embeddings\n",
    "       and varying numbers of RNN cell layers.\"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Map words to an embedding\n",
    "    if not trainable:\n",
    "        model.add(\n",
    "            Embedding(\n",
    "                input_dim=num_words,\n",
    "                output_dim=embedding_matrix.shape[1],\n",
    "                weights=[embedding_matrix],\n",
    "                trainable=False,\n",
    "                mask_zero=True))\n",
    "        model.add(Masking())\n",
    "    else:\n",
    "        model.add(\n",
    "            Embedding(\n",
    "                input_dim=num_words,\n",
    "                output_dim=embedding_matrix.shape[1],\n",
    "                weights=[embedding_matrix],\n",
    "                trainable=True))\n",
    "\n",
    "    # If want to add multiple RNN layers\n",
    "    if rnn_layers > 1:\n",
    "        for i in range(rnn_layers - 1):\n",
    "            model.add(\n",
    "                SimpleRNN(\n",
    "                    rnn_cells,\n",
    "                    return_sequences=True,\n",
    "                    dropout=0.1,\n",
    "                    recurrent_dropout=0.1))\n",
    "\n",
    "    # Add final RNN cell layer\n",
    "    if bi_direc:\n",
    "        model.add(\n",
    "            Bidirectional(\n",
    "                SimpleRNN(\n",
    "                    rnn_cells,\n",
    "                    return_sequences=False,\n",
    "                    dropout=0.1,\n",
    "                    recurrent_dropout=0.1)))\n",
    "    else:\n",
    "        model.add(\n",
    "            SimpleRNN(\n",
    "                rnn_cells,\n",
    "                return_sequences=False,\n",
    "                dropout=0.1,\n",
    "                recurrent_dropout=0.1))\n",
    "#     model.add(Dense(128, activation='relu'))\n",
    "#     # Dropout for regularization\n",
    "#     model.add(Dropout(0.5))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(num_words, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = make_word_level_model(\n",
    "    num_words,\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    rnn_cells=RNN_CELLS,\n",
    "    trainable=False,\n",
    "    rnn_layers=1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "BATCH_SIZE = 2048\n",
    "model_name = 'train-embeddings-rnn-50'\n",
    "model_dir = '../my_models/'\n",
    "\n",
    "def make_callbacks(model_name, save=SAVE_MODEL):\n",
    "    \"\"\"Make list of callbacks for training\"\"\"\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
    "\n",
    "    if save:\n",
    "        callbacks.append(\n",
    "            ModelCheckpoint(\n",
    "                f'{model_dir}{model_name}.h5',\n",
    "                save_best_only=True,\n",
    "                save_weights_only=False))\n",
    "    return callbacks\n",
    "\n",
    "\n",
    "callbacks = make_callbacks(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_evaluate(model_name, return_model=False):\n",
    "    \"\"\"Load in a trained model and evaluate with log loss and accuracy\"\"\"\n",
    "\n",
    "    model = load_model(f'{model_dir}{model_name}.h5')\n",
    "    r = model.evaluate(X_valid, y_valid, batch_size=2048, verbose=1)\n",
    "\n",
    "    valid_crossentropy = r[0]\n",
    "    valid_accuracy = r[1]\n",
    "\n",
    "    print(f'Cross Entropy: {round(valid_crossentropy, 4)}')\n",
    "    print(f'Accuracy: {round(100 * valid_accuracy, 2)}%')\n",
    "\n",
    "    if return_model:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95569/95569 [==============================] - 10s 100us/step\n",
      "Cross Entropy: 4.9522\n",
      "Accuracy: 23.44%\n"
     ]
    }
   ],
   "source": [
    "model = load_and_evaluate(model_name, return_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "def header(text, color='black'):\n",
    "    raw_html = f'<h1 style=\"color: {color};\"><center>' + \\\n",
    "        str(text) + '</center></h1>'\n",
    "    return raw_html\n",
    "\n",
    "\n",
    "def box(text):\n",
    "    raw_html = '<div style=\"border:1px inset black;padding:1em;font-size: 20px;\">' + \\\n",
    "        str(text)+'</div>'\n",
    "    return raw_html\n",
    "\n",
    "\n",
    "def addContent(old_html, raw_html):\n",
    "    old_html += raw_html\n",
    "    return old_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def generate_output(model,\n",
    "                    sequences,\n",
    "                    training_length=50,\n",
    "                    new_words=50,\n",
    "                    diversity=1,\n",
    "                    return_output=False,\n",
    "                    n_gen=1):\n",
    "    \"\"\"Generate `new_words` words of output from a trained model and format into HTML.\"\"\"\n",
    "\n",
    "    # Choose a random sequence\n",
    "    seq = random.choice(sequences)\n",
    "\n",
    "    # Choose a random starting point\n",
    "    seed_idx = random.randint(0, len(seq) - training_length - 10)\n",
    "    # Ending index for seed\n",
    "    end_idx = seed_idx + training_length\n",
    "\n",
    "    gen_list = []\n",
    "\n",
    "    for n in range(n_gen):\n",
    "        # Extract the seed sequence\n",
    "        seed = seq[seed_idx:end_idx]\n",
    "        original_sequence = [idx_word[i] for i in seed]\n",
    "        generated = seed[:] + ['#']\n",
    "\n",
    "        # Find the actual entire sequence\n",
    "        actual = generated[:] + seq[end_idx:end_idx + new_words]\n",
    "\n",
    "        # Keep adding new words\n",
    "        for i in range(new_words):\n",
    "\n",
    "            # Make a prediction from the seed\n",
    "            preds = model.predict(np.array(seed).reshape(1, -1))[0].astype(\n",
    "                np.float64)\n",
    "\n",
    "            # Diversify\n",
    "            preds = np.log(preds) / diversity\n",
    "            exp_preds = np.exp(preds)\n",
    "\n",
    "            # Softmax\n",
    "            preds = exp_preds / sum(exp_preds)\n",
    "\n",
    "            # Choose the next word\n",
    "            probas = np.random.multinomial(1, preds, 1)[0]\n",
    "\n",
    "            next_idx = np.argmax(probas)\n",
    "\n",
    "            # New seed adds on old word\n",
    "            seed = seed[1:] + [next_idx]\n",
    "            generated.append(next_idx)\n",
    "\n",
    "        # Showing generated and actual abstract\n",
    "        n = []\n",
    "\n",
    "        for i in generated:\n",
    "            n.append(idx_word.get(i, '< --- >'))\n",
    "\n",
    "        gen_list.append(n)\n",
    "\n",
    "    a = []\n",
    "\n",
    "    for i in actual:\n",
    "        a.append(idx_word.get(i, '< --- >'))\n",
    "\n",
    "    a = a[training_length:]\n",
    "\n",
    "    gen_list = [\n",
    "        gen[training_length:training_length + len(a)] for gen in gen_list\n",
    "    ]\n",
    "\n",
    "    if return_output:\n",
    "        return original_sequence, gen_list, a\n",
    "\n",
    "    # HTML formatting\n",
    "    seed_html = ''\n",
    "    seed_html = addContent(seed_html, header(\n",
    "        'Seed Sequence', color='darkblue'))\n",
    "    seed_html = addContent(seed_html,\n",
    "                           box(remove_spaces(' '.join(original_sequence))))\n",
    "\n",
    "    gen_html = ''\n",
    "    gen_html = addContent(gen_html, header('RNN Generated', color='darkred'))\n",
    "    gen_html = addContent(gen_html, box(remove_spaces(' '.join(gen_list[0]))))\n",
    "\n",
    "    a_html = ''\n",
    "    a_html = addContent(a_html, header('Actual', color='darkgreen'))\n",
    "    a_html = addContent(a_html, box(remove_spaces(' '.join(a))))\n",
    "\n",
    "    return seed_html, gen_html, a_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'remove_spaces' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-80e58c332a2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m seed_html, gen_html, a_html = generate_output(model, sequences,\n\u001b[0;32m----> 2\u001b[0;31m                                               TRAINING_LENGTH)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_html\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_html\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_html\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-bd9df368c8ce>\u001b[0m in \u001b[0;36mgenerate_output\u001b[0;34m(model, sequences, training_length, new_words, diversity, return_output, n_gen)\u001b[0m\n\u001b[1;32m     80\u001b[0m         'Seed Sequence', color='darkblue'))\n\u001b[1;32m     81\u001b[0m     seed_html = addContent(seed_html,\n\u001b[0;32m---> 82\u001b[0;31m                            box(remove_spaces(' '.join(original_sequence))))\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mgen_html\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'remove_spaces' is not defined"
     ]
    }
   ],
   "source": [
    "seed_html, gen_html, a_html = generate_output(model, sequences,\n",
    "                                              TRAINING_LENGTH)\n",
    "HTML(seed_html)\n",
    "HTML(gen_html)\n",
    "HTML(a_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
